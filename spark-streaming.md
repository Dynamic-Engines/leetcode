
# ğŸ”„ Spark Structured Streaming: Joins & End-to-End Pipeline

---

## ğŸ§  What is Structured Streaming?

Structured Streaming is Apache Sparkâ€™s scalable, high-level, fault-tolerant stream processing engine. It allows you to express streaming logic declaratively, just like batch queries.

---

## ğŸ” Supported Join Types in Structured Streaming

| Join Type          | Stream-Static Join | Stream-Stream Join | Notes                                |
|--------------------|--------------------|---------------------|--------------------------------------|
| Inner Join         | âœ… Supported        | âœ… Supported         | Most commonly used                   |
| Left Outer Join    | âŒ Not Supported    | âœ… Supported         | Requires watermarking                |
| Right Outer Join   | âŒ Not Supported    | âŒ Not Supported     | Not allowed in streaming             |
| Full Outer Join    | âŒ Not Supported    | âŒ Not Supported     | Not allowed in streaming             |

---

## ğŸ“Œ 1. Stream-to-Static Join

**âœ… Allowed: Only Inner Join**

Used when enriching streaming data with small static reference tables.

```scala
val enriched = ordersStream.join(
  broadcast(productsDF),
  "product_id"
)
```

- `productsDF` should be small enough to broadcast
- Used for product, customer, or region lookups

---

## ğŸ“Œ 2. Stream-to-Stream Inner Join

**âœ… Requires Watermarking + Event-Time Condition**

```scala
val ordersW = ordersStream.withWatermark("timestamp", "10 minutes")
val paymentsW = paymentsStream.withWatermark("timestamp", "10 minutes")

val joined = ordersW.join(
  paymentsW,
  expr("""
    ordersW.order_id = paymentsW.order_id AND
    paymentsW.timestamp BETWEEN ordersW.timestamp AND ordersW.timestamp + interval 15 minutes
  """)
)
```

- Spark holds state until watermark expires
- Only matched pairs are emitted

---

## ğŸ“Œ 3. Stream-to-Stream Left Outer Join

**âœ… Also requires watermarking on both sides**

```scala
val joinedLeft = ordersW.join(
  paymentsW,
  expr("""
    ordersW.order_id = paymentsW.order_id AND
    paymentsW.timestamp BETWEEN ordersW.timestamp AND ordersW.timestamp + interval 15 minutes
  """),
  "leftOuter"
)
```

- Emits unmatched left-side rows with nulls on right
- Useful when right events may be delayed or missing

---

## âš™ï¸ Full End-to-End Streaming Pipeline Example

**Goal:**  
- Ingest Kafka `orders` and `payments` streams  
- Enrich with static product data (`products.csv`)  
- Perform stream-to-stream join  
- Write to sink (console or Parquet)  

---

### ğŸ§¾ Step 1: Define Schemas

```scala
val orderSchema = new StructType()
  .add("order_id", StringType)
  .add("product_id", StringType)
  .add("user_id", StringType)
  .add("timestamp", TimestampType)

val paymentSchema = new StructType()
  .add("order_id", StringType)
  .add("amount", DoubleType)
  .add("timestamp", TimestampType)
```

---

### ğŸ“¥ Step 2: Read Kafka Streams

```scala
val ordersStream = spark.readStream
  .format("kafka")
  .option("subscribe", "orders")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load()
  .selectExpr("CAST(value AS STRING)")
  .select(from_json($"value", orderSchema).as("data"))
  .select("data.*")

val paymentsStream = spark.readStream
  .format("kafka")
  .option("subscribe", "payments")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load()
  .selectExpr("CAST(value AS STRING)")
  .select(from_json($"value", paymentSchema).as("data"))
  .select("data.*")
```

---

### ğŸ“š Step 3: Load Static Product Table

```scala
val productsDF = spark.read
  .option("header", "true")
  .csv("products.csv") // Columns: product_id, product_name, category
```

---

### ğŸ”— Step 4: Stream-to-Static Join (Enrich Orders)

```scala
val enrichedOrders = ordersStream.join(
  broadcast(productsDF),
  "product_id"
)
```

---

### ğŸ” Step 5: Stream-to-Stream Join (Orders + Payments)

```scala
val ordersWithWatermark = enrichedOrders.withWatermark("timestamp", "10 minutes")
val paymentsWithWatermark = paymentsStream.withWatermark("timestamp", "10 minutes")

val fullyEnriched = ordersWithWatermark.join(
  paymentsWithWatermark,
  expr("""
    ordersWithWatermark.order_id = paymentsWithWatermark.order_id AND
    paymentsWithWatermark.timestamp BETWEEN ordersWithWatermark.timestamp AND ordersWithWatermark.timestamp + interval 15 minutes
  """)
)
```

---

### ğŸ’¾ Step 6: Write to Sink

#### Option A: Console (for debugging)

```scala
fullyEnriched.writeStream
  .format("console")
  .outputMode("append")
  .option("truncate", false)
  .start()
  .awaitTermination()
```

#### Option B: Parquet (for production)

```scala
fullyEnriched.writeStream
  .format("parquet")
  .option("path", "output/enriched_orders/")
  .option("checkpointLocation", "output/checkpoints/")
  .outputMode("append")
  .start()
```

---

## ğŸ§  Key Concepts

- **Watermarking**:
  - Required for stream-stream joins to control state retention
  - Late data older than watermark is dropped

- **Join Conditions**:
  - Must use event-time columns
  - Must bound time range (`BETWEEN start AND end`)

- **Output Modes**:
  - `append` â€“ for insert-only data (joins, logs)
  - `update` â€“ for aggregations with watermark
  - `complete` â€“ for full re-materialized tables

- **Checkpointing**:
  - Required for recovery and exactly-once guarantees

---

## âœ… Best Practices

- Always define watermarks for stream-stream joins
- Use broadcast joins for stream-static lookups
- Monitor state size in Spark UI or logs
- Avoid full outer joins â€” not supported
- Use Delta/Parquet sinks for analytics

---

## ğŸ§ª Summary

- âœ… **Stream-to-static**: inner join only, use broadcast
- âœ… **Stream-to-stream**: inner and left outer joins allowed
- âŒ **Right/Full outer joins**: not supported in streaming
- âœ… **Watermarks**: mandatory to evict state in joins
- âœ… Use **append mode** for stream joins with checkpoints

---

âœ… You're good to copy this entire block into a .md file or Markdown editor.

Let me know if you'd like:

âœ… PySpark version

âœ… Kafka + Delta + BigQuery pipeline

âœ… Real interview questions based on this scenario (JPMC-style)

âœ… Streaming aggregations (like windowed counts or top-N)


